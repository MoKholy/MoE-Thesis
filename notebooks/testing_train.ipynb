{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from scripts.losses import *\n",
    "from scripts.models import *\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 100\n",
    "num_samples = 1\n",
    "num_experts = 3\n",
    "num_classes = 3\n",
    "hidden_dim = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# create dummy data\n",
    "X = torch.randn(num_samples, input_dim)\n",
    "\n",
    "# create dummy targets\n",
    "y = torch.randint(0, num_classes, (num_samples,))\n",
    "\n",
    "# create one hot encoding for targets\n",
    "y_onehot = F.one_hot(y, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([2])\n",
      "y shape: torch.Size([1])\n",
      "y_onehot: tensor([[0, 0, 1]])\n",
      "y_onehot shape: torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "print(f\"y: {y}\")\n",
    "print(f\"y shape: {y.shape}\")\n",
    "print(f\"y_onehot: {y_onehot}\")\n",
    "print(f\"y_onehot shape: {y_onehot.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get device\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "gating_loss = MSEGatingLoss()\n",
    "expert_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy model\n",
    "model = MixtureOfExperts(input_dim=input_dim, hidden_dim=hidden_dim, num_classes=num_classes, num_experts=num_experts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X type: torch.float32\n",
      "y type: torch.int64\n",
      "y_onehot type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# print types of each data\n",
    "print(f\"X type: {X.dtype}\")\n",
    "print(f\"y type: {y.dtype}\")\n",
    "print(f\"y_onehot type: {y_onehot.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expert_outputs_shape: torch.Size([1, 3, 3])\n",
      "gating_weights_shape: torch.Size([1, 3])\n",
      "mixture_output_shape: torch.Size([1, 3])\n",
      "mixture_out shape: torch.Size([1, 3]) -- y_shape: torch.Size([1])\n",
      "gating_out shape: torch.Size([1, 3]) -- y_onehot_shape: torch.Size([1, 3])\n",
      "\n",
      "output: (tensor([[ 0.2149, -0.0186, -0.2226]], grad_fn=<SumBackward1>), tensor([[0.3642, 0.4012, 0.2346]], grad_fn=<SoftmaxBackward0>), tensor([[[ 0.1415, -0.3374, -0.2104],\n",
      "         [ 0.3098,  0.4765, -0.2298],\n",
      "         [ 0.1663, -0.3705, -0.2293]]], grad_fn=<StackBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# get output from model\n",
    "output = model(X)\n",
    "\n",
    "\n",
    "mixture_out, gating_out, expert_out = output\n",
    "\n",
    "\n",
    "# print shapes of each output\n",
    "print(f\"mixture_out shape: {mixture_out.shape} -- y_shape: {y.shape}\")\n",
    "print(f\"gating_out shape: {gating_out.shape} -- y_onehot_shape: {y_onehot.shape}\")\n",
    "print(f\"\")\n",
    "\n",
    "# get gating loss\n",
    "g_loss = gating_loss(gating_out, y_onehot.float())\n",
    "\n",
    "# get expert loss\n",
    "e_loss = expert_loss(mixture_out, y)\n",
    "\n",
    "# get total loss\n",
    "total_loss = g_loss + e_loss\n",
    "\n",
    "print(f\"output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_loss.dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(f\"total_loss.dtype: {total_loss.dtype}\")\n",
    "\n",
    "# total_loss = total_loss.float()\n",
    "total_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()\n",
    "# test writer dummy data\n",
    "writer.add_scalar(\"test\", 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     36\u001b[0m \u001b[39m# get output from model\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m mixture_out, gating_out, expert_out \u001b[39m=\u001b[39m model(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m     39\u001b[0m \u001b[39m# expert out for debugging\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m# get gating loss\u001b[39;00m\n\u001b[1;32m     42\u001b[0m gating_loss \u001b[39m=\u001b[39m gating_loss(gating_out, true_gating_labels\u001b[39m.\u001b[39mfloat())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Repos/MoE-Thesis/notebooks/../scripts/models.py:57\u001b[0m, in \u001b[0;36mMixtureOfExperts.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     41\u001b[0m     \u001b[39m# # Calculate gating weights\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[39m# gating_weights = self.gating(x)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m     \u001b[39m# calculating gating weights\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     gating_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgating(x)\n\u001b[1;32m     58\u001b[0m     \u001b[39m# calculating expert outputs\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     expert_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([expert(x) \u001b[39mfor\u001b[39;00m expert \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperts], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/Repos/MoE-Thesis/notebooks/../scripts/models.py:25\u001b[0m, in \u001b[0;36mGatingNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 25\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc(x)\n\u001b[1;32m     26\u001b[0m     gating_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(x)\n\u001b[1;32m     27\u001b[0m     \u001b[39mreturn\u001b[39;00m gating_weights\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "epochs = 1 \n",
    "batch_size = 1\n",
    "lr = 0.001\n",
    "# track best f1_score\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "\n",
    "\n",
    "# create dummy dataloader\n",
    "dataloader = DataLoader(dataset=list(zip(X, y_onehot, y)), batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset=list(zip(X, y_onehot, y)), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "best_f1_score = 0.0\n",
    "writer = SummaryWriter()\n",
    "for epoch in tqdm(range(epochs), desc=\"Epochs\"):\n",
    "\n",
    "    \n",
    "    # set model to train\n",
    "    model.train()\n",
    "\n",
    "    # track losses, predictions and labels\n",
    "    total_expert_loss = 0.0\n",
    "    total_gating_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # loop over data from dataloader\n",
    "\n",
    "    for input, true_gating_labels, labels in tqdm(dataloader, desc=\"Batches\", leave=False):\n",
    "        # get data to device\n",
    "        input = input.to(device)\n",
    "        true_gating_labels = true_gating_labels.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # get output from model\n",
    "        mixture_out, gating_out, expert_out = model(input)\n",
    "        \n",
    "        # expert out for debugging\n",
    "\n",
    "        # get gating loss\n",
    "        gating_loss = gating_loss(gating_out, true_gating_labels.float())\n",
    "        total_gating_loss += gating_loss.item()\n",
    "        # get expert loss\n",
    "        expert_loss = expert_loss(mixture_out, labels)\n",
    "        total_expert_loss += expert_loss.item()\n",
    "        # get total loss\n",
    "        total_loss = gating_loss + expert_loss\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        # backpropagate\n",
    "        total_loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        # update scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # calculate predictions for accuracy and F1 score\n",
    "        _, preds = torch.max(mixture_out, dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # calculate average losses for epoch\n",
    "    avg_expert_loss = total_expert_loss / len(dataloader)\n",
    "    avg_gating_loss = total_gating_loss / len(dataloader)\n",
    "\n",
    "    # calculate accuracy and F1 score\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    # write to tensorboard\n",
    "    writer.add_scalar(\"Train/Avg_Train_Expert_Loss\", avg_expert_loss, epoch)\n",
    "    writer.add_scalar(\"Train/Avg_Train_Gating_Loss\", avg_gating_loss, epoch)\n",
    "    writer.add_scalar(\"Train/Train_Accuracy\", accuracy, epoch)\n",
    "    writer.add_scalar(\"Train/Train_F1_Score\", f1, epoch)\n",
    "\n",
    "    # perform validation\n",
    "\n",
    "    # set model to eval\n",
    "    model.eval()\n",
    "\n",
    "    # track losses, predictions and labels for validation\n",
    "    total_expert_loss_val = 0.0\n",
    "    total_gating_loss_val = 0.0\n",
    "    all_preds_val = []\n",
    "    all_labels_val = []\n",
    "\n",
    "    # loop over data from dataloader\n",
    "    for input, true_gating_labels, labels in tqdm(dataloader_val, desc=\"Validation\", leave=False):\n",
    "        # get data to device\n",
    "        input = input.to(device)\n",
    "        true_gating_labels = true_gating_labels.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # get output from model\n",
    "        mixture_out, gating_out, expert_out = model(input)\n",
    "        # get gating loss\n",
    "        gating_loss = gating_loss(gating_out, true_gating_labels.float())\n",
    "        total_gating_loss_val += gating_loss.item()\n",
    "        # get expert loss\n",
    "        expert_loss = expert_loss(mixture_out, labels)\n",
    "        total_expert_loss_val += expert_loss.item()\n",
    "\n",
    "        # calculate predictions for accuracy and F1 score\n",
    "        _, preds = torch.max(mixture_out, dim=1)\n",
    "        all_preds_val.extend(preds.cpu().numpy())\n",
    "        all_labels_val.extend(labels.cpu().numpy())\n",
    "\n",
    "    # calculate average losses for epoch\n",
    "    avg_expert_loss_val = total_expert_loss_val / len(dataloader_val)\n",
    "    avg_gating_loss_val = total_gating_loss_val / len(dataloader_val)\n",
    "\n",
    "    # calculate accuracy and F1 score\n",
    "    accuracy_val = accuracy_score(all_labels_val, all_preds_val)\n",
    "    f1_val = f1_score(all_labels_val, all_preds_val, average=\"macro\")\n",
    "\n",
    "    # write to tensorboard\n",
    "    writer.add_scalar(\"Train/Avg_Val_Expert_Loss\", avg_expert_loss_val, epoch)\n",
    "    writer.add_scalar(\"Train/Avg_Val_Gating_Loss\", avg_gating_loss_val, epoch)\n",
    "    writer.add_scalar(\"Train/Val_Accuracy\", accuracy_val, epoch)\n",
    "    writer.add_scalar(\"Train/Val_F1_Score\", f1_val, epoch)\n",
    "\n",
    "    # check if F1 score is best\n",
    "    if f1_val > best_f1_score:\n",
    "        # save model\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        # update best F1 score\n",
    "        best_f1_score = f1\n",
    "    \n",
    "writer.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
